\documentclass{article}
\usepackage{cmap}  % should be before fontenc
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{listings}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[pdftex,colorlinks=true,linkcolor=blue,urlcolor=red,unicode=true,hyperfootnotes=false,bookmarksnumbered]{hyperref}
\usepackage{indentfirst}

% так можно определять команду для повторяющихся обозначений,
% чтобы не набирать каждый раз заново
\newcommand{\E}{\ensuremath{\mathsf{E}}}  % матожидание
\newcommand{\D}{\ensuremath{\mathsf{D}}}  % дисперсия
\newcommand{\Prb}{\ensuremath{\mathsf{P}}}  % вероятностная мера

\newcommand{\eps}{\varepsilon}  % нормальная буква эпсилон
\renewcommand{\phi}{\varphi}  % нормальная буква фи

\renewcommand{\le}{\leqslant}  % нормальный знак <=
\renewcommand{\leq}{\leqslant}  % нормальный знак <=
\renewcommand{\ge}{\geqslant}  % нормальный знак >=
\renewcommand{\geq}{\geqslant}  % нормальный знак >=

\newtheorem{lemma}{Лемма}  % создаёт команд для лемм, можно сделать так же для любого другого вида утверждений

\pagestyle{myheadings}
\markright{Iaroslav Khripkov\hfill}  % <- здесь нужно подставить свои имя и фамилию

\begin{document}

\section{PDF with Bayes rule}

\subsection*{problem statement}
A robot uses a range sensor that can measure ranges from 0m and 3m. For simplicity, assume that the actual ranges are distributed uniformly in this interval. Unfortunately, the sensor can be faulty. When the sensor is faulty, it constantly outputs a range below 1m, regardless of the actual range in the sensor's measurement cone. We know that the prior probability for a sensor to be faulty is  $p=0.01$ .
Suppose the robot queried it's sensor N times, and every single time the measurement value is below 1m. What is the posterior probability of a sensor fault, for  $N=1,2,\dots,10$ . Formulate the corresponding probabilistic model.
\subsection*{solution}
Let's describe PDF of prior distributions:
$$ p_{gt}(x) = U(0,3) $$
Therefore observation probability density function consists of two mutually exclusive events: sensor is faulty and it's probability = 0.01, and when sensor is working properly and actual distance is less than 1m. 
It implies the following equation for PDF of observed distance:
$$ p_{obs}(x) = 0.99*p_{gt}(x) + 0.01 * I[x<1m]$$

\subsubsection*{N=1}

Let's define several events:


\textbf{A - sensor is faulty}


\textbf{B - observed distance is less than 1m}


Now let's define the probability of sensor fault when observed distance < 1m
For this task we will use Bayes theorem
$$ P(A) = 0.01 \text{ (from the statement)}$$
$$ P(B) = P_{obs}(X<1) = 0.99 * \frac{1}{3} + 0.01 = 0.34 $$


$P(B|A) = 1$ (since the faulty sensor always "measures" distance less than 1 meter)


So, $P(A|B)=\frac{P(B|A)P(A)}{P(B)} = \frac{1 * 0.01}{0.34} = 0.0294$


\subsubsection*{N measurements}

In this case our P(A) and P(B|A) are the same as in the first part. But here is what happening to our P(B). Now we measured <1m N times and calculating the probability of it or sensor was faulty. 
$$ P(B) = 0.99 * (\frac{1}{3})^{N} + 0.01 $$

Final formula now looks like this:
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} = \frac{0.01}{0.99 (\frac{1}{3})^{N} + 0.01}$$


\section{Weather transitions}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 \text{Today/Tomorrow} & sunny & cloudy & rainy  \\ 
 \hline
 sunny & 0.8 & 0.2 & 0 \\ 
 \hline
 cloudy & 0.4 & 0.4 & 0.2 \\ 
 \hline
 rainy & 0.2 & 0.6 & 0.2 \\ 
 \hline
\end{tabular}
\end{center}
\subsection{Probabilities}
If today is sunny then tomorrow will be cloudy with probability of 0.2, after a cloudy day we will have another cloudy day with probability of 0.4, then we will have rain with 0.2 probability.
The resulting probability is: $p=0.2*0.4*0.2$

\subsection{Code}
\begin{lstlisting}
#include <random>
#include <iostream>

using namespace std;

enum Day {
    Sunny,
    Cloudy,
    Rainy
};

double getZeroOneRange(mt19937 &gen) {
    return (double) gen() / std::mt19937::max();
}

string dayToString(Day day) {
    if (day == Sunny) {
        return "Sunny";
    }
    if (day == Cloudy) {
        return "Cloudy";
    }
    return "Rainy";

}

Day sim(Day cur, mt19937 &gen) {
    double prob = getZeroOneRange(gen);
    if (cur == Sunny) {
        if (prob < .8) {
            return Sunny;
        }
        return Cloudy;
    }
    if (cur == Cloudy) {
        if (prob < .4) {
            return Sunny;
        }
        if (prob < .8) {
            return Cloudy;
        }
        return Rainy;
    }
    if (prob < 0.2) {
        return Sunny;
    }
    if (prob < 0.8) {
        return Cloudy;
    }
    return Rainy;
}

int main() {
    std::random_device rd;
    mt19937 gen(rd());
    long long s = 0, c = 0, r = 0;
    string start;
    long long n;
    Day cur = Sunny;
    cout << "Enter the today's weather: ";
    cin >> start;
    if (start == "cloudy") {
        cur = Cloudy;
    } else if (start == "rainy") {
        cur = Rainy;
    }
    cout << "Enter number of iterations: ";
    cin >> n;
    Day first = cur;
    for (long long i = 0; i < n; ++i) {
        //cout<<"DAY "<<i<<": "<< dayToString(cur)<<'\n';
        if (cur == Sunny) {
            s++;
        } else if (cur == Cloudy) {
            c++;
        } else {
            r++;
        }
        cur = sim(cur, gen);
    }
    cout << "Probabilities with first given weather: " << dayToString(first) << " and " << n << " iterations" << '\n';
    cout << "Probability of sunny day: " << (double) s / n << '\n';
    cout << "Probability of cloudy day: " << (double) c / n << '\n';
    cout << "Probability of rainy day: " << (double) r / n << '\n';
    return 0;
}
\end{lstlisting}


\subsection{Stationary distribution}
Using the code above we can get such results:

\text{}

Probabilities with first given weather: Sunny and 100000 iterations

Probability of sunny day: 0.64287

Probability of cloudy day: 0.285

Probability of rainy day: 0.07213

\text{}

Probabilities with first given weather: Cloudy and 100000 iterations

Probability of sunny day: 0.64304

Probability of cloudy day: 0.28568

Probability of rainy day: 0.07128

\text{}

Probabilities with first given weather: Rainy and 100000 iterations

Probability of sunny day: 0.64114

Probability of cloudy day: 0.28679

Probability of rainy day: 0.07207

\subsection{Closed-form stationary distribution}
Solution for stationary distribution finding problem is used to be written like that:

$\pi$ - is our target stationary distribution

$P$ - is our transition matrix

Then we have to work with this equation:
$$\pi^T = \pi^TP$$
It is easy to recognize here problem of finding eigenvalues of a matrix.
So our next step is to convert our given transition table to an expected transition matrix format. All we need to do is to transpose our matrix.
Our $P$ after it would look like this:
$$P=\begin{pmatrix}0.8 & 0.2 & 0 \\
0.4 & 0.4 & 0.2 \\
0.2 & 0.6 & 0.2 \\
\end{pmatrix}^T = \begin{pmatrix}0.8 & 0.4 & 0.2 \\
0.2 & 0.4 & 0.6 \\
0 & 0.2 & 0.2 \\
\end{pmatrix}$$

The next step is to find eigenvector for this matrix with given $eigenvalue = 1$
$$\pi^T\begin{pmatrix}\-0.2 & 0.4 & 0.2 \\
0.2 & -0.6 & 0.6 \\
0 & 0.2 & 0.2 \end{pmatrix} = \theta $$


After solving this equations our $\pi = (9,4,1)$, now we need to normalize it $\pi=\frac{(9,4,1)}{9+4+1} = (\frac{9}{14},\frac{4}{14},\frac{1}{14})$

After all, $\pi = (0.6429, 0.2857, 0.0714)$ which is close to our empirical results
\subsection{Entropy of stationary distribution}
Let's write entropy formula first:
$$H(X)=-\sum_{i \in x}p(i)\log_2(p(i)$$
Our result would be:
$$H(X)=1.1981$$
\subsection{Yesterday's weather}
Since we know the prior probabilities of weather, we can easily apply Bayes rule to our task.
Let's describe our random variables:
$$A - \text{Yesterday's weather}$$
$$B - \text{Today's weather}$$
We know probability distribution for $P(B|A)$, so now we can find $P(A|B)$
$P(A) = P(B) = \pi \text{ from 2.4}$
$P(B|A) \text{is our transition model}$
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
Time to find every value:
$$P(A_s|B_s) = \frac{P(B_s|A_s)P(A_s)}{P(B_s)} = \frac{0.8*9/14}{9/14} = 0.8$$
$$P(A_c|B_s) = \frac{P(B_s|A_c)P(A_c)}{P(B_s)} = \frac{0.4*4/14}{9/14} = 0.1778$$
$$P(A_r|B_s) = \frac{P(B_s|A_r)P(A_r)}{P(B_s)} = \frac{0.2*1/14}{9/14} = 0.0222$$
$$P(A_s|B_c) = \frac{P(B_c|A_s)P(A_s)}{P(B_c)} = \frac{0.2*9/14}{4/14} = 0.45 $$
$$P(A_c|B_c) = \frac{P(B_c|A_c)P(A_c)}{P(B_c)} = \frac{0.4*4/14}{4/14} = 0.4$$
$$P(A_r|B_c) = \frac{P(B_c|A_r)P(A_r)}{P(B_c)} = \frac{0.6*1/14}{4/14} = 0.15$$
$$P(A_s|B_r) = \frac{P(B_r|A_s)P(A_s)}{P(B_r)} = \frac{0*9/14}{1/14} = 0$$
$$P(A_c|B_r) = \frac{P(B_r|A_c)P(A_c)}{P(B_r)} = \frac{0.2*4/14}{1/14} = 0.8$$
$$P(A_r|B_r) = \frac{P(B_r|A_r)P(A_r)}{P(B_r)} = \frac{0.2*1/14}{1/14} = 0.2$$

Final probability table:
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 \text{Today/Yesterday} & sunny & cloudy & rainy  \\ 
 \hline
 sunny & 0.8 & 0.1778 & 0.0222 \\ 
 \hline
 cloudy & 0.45 & 0.4 & 0.15 \\ 
 \hline
 rainy & 0 & 0.8 & 0.2 \\ 
 \hline
\end{tabular}
\end{center}
\subsection{Seasons}
If we add seasons to our model, it would violate Markov property of the process for one simple reason. Our predictions should rely only on present data, but we cannot predict weather this way by the end of the season, since we apply the "next season" model on "this season" data

\section{Bayes filter}
\begin{center}
\textbf{Probs table}
\end{center}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 \text{Real(x)/Observed(z)} & sunny & cloudy & rainy  \\ 
 \hline
 sunny & 0.6 & 0.4 & 0 \\ 
 \hline
 cloudy & 0.3 & 0.7 & 0 \\ 
 \hline
 rainy & 0 & 0 & 1 \\ 
 \hline
\end{tabular}
\end{center}

\subsection{Day 5}

By applying Bayes rule and Markov assumption
$$P(x_5|x_1, z_{2..5}) = \eta P(z_5|x_5,x_1,z_{2..4})P(x_5|x_1,z_{2..4})$$
$$\implies$$
$$P(x_5|x_1,z_{2..4}) = \sum_{x_4}P(x_4,x_5|x_1, z_{2..4}) $$
$$P(x_5|x_1, z_{2..4}) = \sum_{x_4}P(x_5|x_4,x_1,z_{2..4})P(x_4|x_1,z_{2..4})$$
In the equation above we can find interesting fact in our case, that $P(x_4|z_{2..4}=1$ as mentioned in probabilities table because $z_4=rainy \implies x_4=rainy$


With this fact:
$$P(x_5|x_1, z_{2..4}) = P(x_5|x_4=rainy)P(x_5|x_4=rainy) $$
$$\implies$$
$$P(x_5|x_1,z_{2..4}) = P(x_5|x_4=rainy)\cdot 1 = P(x_5|x_4=rainy)$$
Time to bring our original equation back:
$$P(x_5|x_1,z_{2..5}) =\eta P(z_5|x_5, x1, z_{2..4})P(x_5|x_1, z_{2..4})$$
$$=\eta P(z_5|x_5, x_1, z_{2..4})P(x_5|x_4=rainy)=\eta P(z_5|x_5)P(x_5|x_4=rainy)$$
$$ \eta  = \sum_{x_5}P(z_5=sunny|x_5)P(x_5|x_4=rainy) $$
$$ \eta = 0.6 \cdot 0.2 + 0.3 \cdot 0.6  + 0 \cdot 0.2 = 0.12+0.18 = 0.3 $$

Back to original formula:
$$P(x_5|x_1,z_{2..5}) = \frac{P(z_5=sunny|x_5=sunny)P(x_5=sunny|x_4=rainy)}{0.3} = \frac{0.6 \cdot 0.2}{0.3} = 0.4$$
If this explanation wasn't quite clear, you should probably refresh your Bayes rule and conditional independence knowledges (e.g 2.31 in the book)
\subsection{Probabilities}
\subsubsection{No time machine}
sunny, sunny, rainy
\textbf{N=2}
$$P(x_2|z_2, x_1) =  \eta P(z_2|x_2, x_1)P(x_2|x_1) = \eta P(z_2|x_2, x_1)P(x_2|x_1) $$
$$P(x_2|z_2=sunny, x_1=sunny) =\eta P(z_2=sunny|x_2)P(x_2|x_1=sunny) $$

$$P(x_2|z_2=sunny, x_1=sunny) =\eta \begin{pmatrix}
0.6 \\
 0.4 \\ 
 0
\end{pmatrix} \cdot \begin{pmatrix}
0.8 \\ 0.2 \\ 0
\end{pmatrix} = \eta \begin{pmatrix}
0.48 \\ 0.08 \\0
\end{pmatrix}  = \begin{pmatrix}
8/9 \\ 1/9 \\ 0
\end{pmatrix} $$

\textbf{N=3}
$$P(x_3|z_{2:3}, x_1) =  \eta P(z_3|x_3,z_2)P(x_3|x_1,z_2) = \eta P(z_3|x_3)P(x_3|x_1,z_2) $$
$$= \eta P(z_3|x_3) \sum_{x_2}P(x_3|x_2) P(x_2|x_1,z_2) =\eta P(z_3|x_3)\cdot
\begin{pmatrix}
0.8 & 0.2 & 0 \\
0.4 & 0.4 & 0.2 \\
0.2 & 0.6 & 0.2
\end{pmatrix} \times  \begin{pmatrix}
8/9 \\ 1/9 \\ 0
\end{pmatrix} $$


$$P(x_2|z_2=sunny, x_1=sunny) =\eta \begin{pmatrix}
0.6 \\
 0.4 \\ 
 0
\end{pmatrix} \cdot \begin{pmatrix}
0.8 \\ 0.2 \\ 0
\end{pmatrix} = \eta \begin{pmatrix}
0.48 \\ 0.08 \\0
\end{pmatrix}  = \begin{pmatrix}
8/9 \\ 1/9 \\ 0
\end{pmatrix} $$

\subsection{most likely sequence}
\end{document}
